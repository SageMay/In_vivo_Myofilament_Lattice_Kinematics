{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal \n",
    "Save the test and train trial data for ML in formats that will be easy to load in xGBoost model. \n",
    "This is specifically geared to create a training file that will contain the data from all trials, except trial X. It is correspondingly named after trial X, and the corresponding test file (also named for trial X) contains data only from trial X.\n",
    "Hence for the extrapolating scripts the training set for moth X is used along with the test set for moth X. \n",
    "Meanwhile for the interpolating script only the test set for moth X is used. Within the training of the model 75% of the data is used and 25% is withheld. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score # This seems to only work for classification\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the trials that we have good data for \n",
    "acceptable_trials = ['m07_t01_15', 'm07_t03_15', 'm07_t06_15','m10_t02_16','m11_t02_16','m11_t04_16',\n",
    "                   'm12_t02_16','m14_t05_16', 'm14_t03_16', 'm15_t01_16', 'm15_t03_16']\n",
    "\n",
    "# Split the data into test and train sets\n",
    "test_trials = []\n",
    "train_trials = []\n",
    "for i in np.arange(0,len(acceptable_trials)):\n",
    "    test_trials.append(acceptable_trials[i])\n",
    "    train_trials.append((acceptable_trials[:(i)] + acceptable_trials[(i+1):]))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on N-1 of N trials and test on the removed trial\n",
    "\n",
    "test_column = 'D10'\n",
    "predictor_columns = ['Wingbeat_freq', 'M6_c', 'I20_I10', 'M3_i', 'A59_c']\n",
    "\n",
    "for i in np.arange(0,len(train_trials)): \n",
    "    # Prep the data\n",
    "    # Base case\n",
    "    trial = train_trials[i][0]\n",
    "    d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "    d = d[['D10', 'D11', 'D20', 'I20I11_I10', 'I20_I10',\n",
    "            'fitting_error', 'seconds', 'tif_num', 'M3_c', 'M6_c', 'A51_c', 'A59_c',\n",
    "            'M3_i', 'M6_i', 'A59_i', 'A51_i', 'Sum', 'peaks', 'ISI']]\n",
    "\n",
    "    d = d.interpolate() # Fill any nan values\n",
    "    d = d[['D10', 'I20_I10', 'M3_i', 'A59_c', 'M6_c']]\n",
    "    d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "\n",
    "    columns = ['M3_i', 'M6_c', 'I20_I10', 'A59_c']\n",
    "    shift_column_names = []\n",
    "    for col in columns:\n",
    "        d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "        shift_column_names.append(col + '_' + str(0))\n",
    "\n",
    "    for j in np.arange(1,11):\n",
    "        for col in columns:\n",
    "            d[col + '_' + str(j)] = d[col + '_' + str(j - 1)].shift(-1)\n",
    "            shift_column_names.append(col + '_' + str(j))\n",
    "\n",
    "    d = d.dropna()\n",
    "\n",
    "    # Subset the data into predictors (X) and predicted (y)\n",
    "    X = (d[columns + shift_column_names])\n",
    "    y = (d[['D10']])\n",
    "\n",
    "\n",
    "    # Subsequent cases\n",
    "\n",
    "    # Create training set\n",
    "    for trial in train_trials[i][1:]: \n",
    "        d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "        d = d[['D10', 'D11', 'D20', 'I20I11_I10', 'I20_I10',\n",
    "               'fitting_error', 'seconds', 'tif_num', 'M3_c', 'M6_c', 'A51_c', 'A59_c',\n",
    "                'M3_i', 'M6_i', 'A59_i', 'A51_i', 'Sum', 'peaks', 'ISI']]\n",
    "\n",
    "        d = d.interpolate()\n",
    "        d = d[['D10', 'I20_I10', 'M3_i', 'A59_c', 'M6_c']]\n",
    "        d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "        shift_column_names = []\n",
    "        for col in columns:\n",
    "            d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "            shift_column_names.append(col + '_' + str(0)) # This was commented... I Don't know why?!\n",
    "\n",
    "        for j in np.arange(1,11):\n",
    "            for col in columns:\n",
    "                d[col + '_' + str(j)] = d[col + '_' + str(j - 1)].shift(-1)\n",
    "                shift_column_names.append(col + '_' + str(j))\n",
    "        d = d.dropna()\n",
    "\n",
    "        # Subset the data into predictors (X) and predicted (y)\n",
    "        X = X.append(d[columns + shift_column_names])\n",
    "        y = y.append(d[['D10']])\n",
    "\n",
    "    X = X.reset_index(drop = 'True')\n",
    "    y = y.reset_index(drop = 'True')\n",
    "    \n",
    "    X.to_csv('./MLFormattedData/Train/' + test_trials[i] + '_TrainX.csv', index = False)\n",
    "    y.to_csv('./MLFormattedData/Train/' + test_trials[i] + '_TrainY.csv', index = False)\n",
    "    \n",
    "    del X,y\n",
    "\n",
    "    # Use the last df as a test set\n",
    "    trial = test_trials[i]\n",
    "    d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "    d = d[['D10', 'D11', 'D20', 'I20I11_I10', 'I20_I10',\n",
    "            'fitting_error', 'seconds', 'tif_num', 'M3_c', 'M6_c', 'A51_c', 'A59_c',\n",
    "           'M3_i', 'M6_i', 'A59_i', 'A51_i', 'Sum', 'peaks', 'ISI']]\n",
    "    d = d.interpolate()\n",
    "    d = d[['D10', 'I20_I10', 'M3_i', 'A59_c', 'M6_c']]\n",
    "    d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "\n",
    "    # Create time shifted predictor data\n",
    "    for col in columns:\n",
    "        d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "    for j in np.arange(1,11):\n",
    "        for col in columns:\n",
    "            d[col + '_' + str(j)] = d[col + '_' + str(j - 1)].shift(-1)\n",
    "    d = d.dropna()\n",
    "\n",
    "    X = (d[columns + shift_column_names])\n",
    "    y = (d[['D10']])\n",
    "    \n",
    "    X.to_csv('./MLFormattedData/Test/' + test_trials[i] + '_TestX.csv', index = False)\n",
    "    y.to_csv('./MLFormattedData/Test/' + test_trials[i] + '_TestY.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
