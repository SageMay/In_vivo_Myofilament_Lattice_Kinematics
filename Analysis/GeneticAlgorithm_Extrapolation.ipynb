{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic algorithm taken from: https://towardsdatascience.com/hyperparameter-tuning-in-xgboost-using-genetic-algorithm-17bd2e581b17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initilialize_poplulation(numberOfParents):\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "    for i in range(numberOfParents):\n",
    "        print(i)\n",
    "        learningRate[i] = round(random.uniform(0.001, 1), 2)\n",
    "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
    "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "    \n",
    "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(y_true, y_pred):\n",
    "    fitness = round((1/mean_squared_error(y_true, y_pred)), 7)\n",
    "    return fitness  # train the data and find fitness score; I modified fitness to 1/MSE -- therefore we still want the highest \"fitness\"\n",
    "\n",
    "def train_population(population, TrainX, TrainY, TestX, TestY):\n",
    "    fScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        param = { 'objective':'reg:squarederror',\n",
    "              'learning_rate': population[i][0],\n",
    "              'n_estimators': population[i][1], \n",
    "              'max_depth': int(population[i][2]), \n",
    "              'min_child_weight': population[i][3],\n",
    "              'gamma': population[i][4], \n",
    "              'subsample': population[i][5],\n",
    "              'colsample_bytree': population[i][6],\n",
    "              'seed': 24} # Maybe add alpha at some point... \n",
    "        num_round = 100\n",
    "        \n",
    "        # Put data into dmatrix format\n",
    "        xgDMatrix = xgb.DMatrix(TrainX, TrainY) #create Dmatrix\n",
    "        xgbDMatrixTest = xgb.DMatrix(TestX, TestY)\n",
    "        dMatrixtest = xgbDMatrixTest\n",
    "        dMatrixTrain = xgDMatrix\n",
    "        # This is where we need to put the regressor bit\n",
    "        #xg_reg = xgb.XGBRegressor(**param, num_boost_round = num_round)\n",
    "        #xg_reg.fit(TrainX, TrainY)\n",
    "        #preds = xg_reg.predict(TestX)\n",
    "        \n",
    "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
    "        preds = xgbT.predict(dMatrixtest)\n",
    "        #preds = preds>0.5 # I have no idea why this is here; it creates an array of \"True\" the size of preds; however preds is needed to calculate the fitness score\n",
    "        #print('test y ', TestY, 'preds ', preds)\n",
    "        fScore.append(fitness(TestY, preds))\n",
    "        # I think maybe they're clearing preds, which shouldn't be needed... but I tried it down here\n",
    "        preds = preds>0.5\n",
    "    return fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select parents for mating\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) #create an array to store fittest parents\n",
    "    \n",
    "    # find the top best performing parents\n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness)) \n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1  # set this value to negative, in case of F1-score, so this parent is not selected again; changed to nan\n",
    "    return selectedParents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mate these parents to create children having parameters from these parents (we are using uniform crossover method)\n",
    "'''\n",
    "\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) #get all the index\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly\n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    '''\n",
    "    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values\n",
    "    will be picked from the indexes, which were randomly selected above. \n",
    "    '''\n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        # find parent 1 index \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        # find parent 2 index\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        # insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        # insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(crossover, numberOfParameters):\n",
    "    \n",
    "    # Define minimum and maximum values allowed for each parameter\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    \n",
    "    minMaxValue[0,:] = [0.01, 1.0] #min/max learning rate\n",
    "    minMaxValue[1, :] = [10, 2000] #min/max n_estimator\n",
    "    minMaxValue[2, :] = [1, 15] #min/max depth\n",
    "    minMaxValue[3, :] = [0, 10.0] #min/max child_weight\n",
    "    minMaxValue[4, :] = [0.01, 10.0] #min/max gamma\n",
    "    minMaxValue[5, :] = [0.01, 1.0] #min/maxsubsample\n",
    "    minMaxValue[6, :] = [0.01, 1.0] #min/maxcolsample_bytree\n",
    " \n",
    "    # Mutation changes a single gene in each offspring randomly.\n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, 7, 1)\n",
    "    print(parameterSelect)\n",
    "    if parameterSelect == 0: #learning_rate\n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 1: #n_estimators\n",
    "        mutationValue = np.random.randint(-200, 200, 1)\n",
    "    if parameterSelect == 2: #max_depth\n",
    "        mutationValue = np.random.randint(-5, 5, 1)\n",
    "    if parameterSelect == 3: #min_child_weight\n",
    "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "    if parameterSelect == 4: #gamma\n",
    "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "    if parameterSelect == 5: #subsample\n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 6: #colsample\n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "  \n",
    "    # indtroduce mutation by changing one parameter, and set to max or min if it goes out of range\n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0] \n",
    "    return crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_trials = ['m07_t01_15', 'm07_t03_15', 'm07_t06_15','m10_t02_16','m11_t02_16','m11_t04_16',\n",
    "                   'm12_t02_16','m14_t05_16', 'm14_t03_16', 'm15_t01_16', 'm15_t03_16']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m07_t01_15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m07_t03_15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m07_t06_15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m10_t02_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m11_t02_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m11_t04_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m12_t02_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>m14_t05_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m14_t03_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>m15_t01_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>m15_t03_16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Trial  learning_rate  n_estimators  max_depth  min_child_weight  \\\n",
       "0   m07_t01_15            NaN           NaN        NaN               NaN   \n",
       "1   m07_t03_15            NaN           NaN        NaN               NaN   \n",
       "2   m07_t06_15            NaN           NaN        NaN               NaN   \n",
       "3   m10_t02_16            NaN           NaN        NaN               NaN   \n",
       "4   m11_t02_16            NaN           NaN        NaN               NaN   \n",
       "5   m11_t04_16            NaN           NaN        NaN               NaN   \n",
       "6   m12_t02_16            NaN           NaN        NaN               NaN   \n",
       "7   m14_t05_16            NaN           NaN        NaN               NaN   \n",
       "8   m14_t03_16            NaN           NaN        NaN               NaN   \n",
       "9   m15_t01_16            NaN           NaN        NaN               NaN   \n",
       "10  m15_t03_16            NaN           NaN        NaN               NaN   \n",
       "\n",
       "    gamma  subsample  colsample_bytree  \n",
       "0     NaN        NaN               NaN  \n",
       "1     NaN        NaN               NaN  \n",
       "2     NaN        NaN               NaN  \n",
       "3     NaN        NaN               NaN  \n",
       "4     NaN        NaN               NaN  \n",
       "5     NaN        NaN               NaN  \n",
       "6     NaN        NaN               NaN  \n",
       "7     NaN        NaN               NaN  \n",
       "8     NaN        NaN               NaN  \n",
       "9     NaN        NaN               NaN  \n",
       "10    NaN        NaN               NaN  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty dataframe to store the optimal hyper parameters determined by the genetic algorithm\n",
    "\n",
    "HypParams= pandas.DataFrame({'Trial' : acceptable_trials})  \n",
    "a = np.full((len(HypParams)), np.nan)\n",
    "column_names = ['learning_rate', 'n_estimators', 'max_depth', 'min_child_weight', 'gamma', 'subsample','colsample_bytree']\n",
    "for col in column_names: \n",
    "    HypParams[col] = a\n",
    "\n",
    "HypParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 3.2547504\n",
      "[2]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 3.2547504\n",
      "[4]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 3.2547504\n",
      "[3]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 3.7323698\n",
      "[1]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 3.7323698\n",
      "[0]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 3.7323698\n",
      "[6]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 3.7323698\n",
      "[4]\n",
      "Best fitness is = 3.7323698\n",
      "Best parameters are:\n",
      "learning_rate 0.31\n",
      "n_estimators 198.0\n",
      "max_depth 5\n",
      "min_child_weight 10.0\n",
      "gamma 7.95\n",
      "subsample 0.85\n",
      "colsample_bytree 0.91\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 0.9512963\n",
      "[3]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 0.9659019\n",
      "[6]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 0.9659019\n",
      "[6]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 0.9659019\n",
      "[6]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 0.9659019\n",
      "[5]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 0.9659019\n",
      "[2]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 0.9659019\n",
      "[3]\n",
      "Best fitness is = 0.9659019\n",
      "Best parameters are:\n",
      "learning_rate 0.11\n",
      "n_estimators 10.0\n",
      "max_depth 4\n",
      "min_child_weight 10.0\n",
      "gamma 0.61\n",
      "subsample 0.63\n",
      "colsample_bytree 0.92\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 0.6094171\n",
      "[4]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 0.665399\n",
      "[4]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 0.665399\n",
      "[6]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 0.665399\n",
      "[1]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 0.6741732\n",
      "[0]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 0.6741732\n",
      "[6]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 0.6741732\n",
      "[4]\n",
      "Best fitness is = 0.6888331\n",
      "Best parameters are:\n",
      "learning_rate 0.05\n",
      "n_estimators 26.0\n",
      "max_depth 3\n",
      "min_child_weight 6.69\n",
      "gamma 7.42\n",
      "subsample 0.76\n",
      "colsample_bytree 0.13\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 0.4842221\n",
      "[4]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 0.4842221\n",
      "[0]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 0.4879964\n",
      "[6]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 0.4879964\n",
      "[1]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 0.4945731\n",
      "[0]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 0.4945731\n",
      "[1]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 0.4945731\n",
      "[4]\n",
      "Best fitness is = 0.4945731\n",
      "Best parameters are:\n",
      "learning_rate 0.21\n",
      "n_estimators 54.0\n",
      "max_depth 4\n",
      "min_child_weight 5.72\n",
      "gamma 4.68\n",
      "subsample 0.67\n",
      "colsample_bytree 0.79\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 1.6971304\n",
      "[5]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 1.8852693\n",
      "[2]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 1.9084202\n",
      "[1]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 1.9084202\n",
      "[6]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 1.9084202\n",
      "[2]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 1.9084202\n",
      "[2]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 1.9084202\n",
      "[5]\n",
      "Best fitness is = 1.9146835\n",
      "Best parameters are:\n",
      "learning_rate 0.06\n",
      "n_estimators 217.0\n",
      "max_depth 1\n",
      "min_child_weight 9.41\n",
      "gamma 7.24\n",
      "subsample 1.0\n",
      "colsample_bytree 0.36\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 0.9744831\n",
      "[5]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 1.0173689\n",
      "[3]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 1.0173689\n",
      "[6]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 1.0173689\n",
      "[3]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 1.0173689\n",
      "[1]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 1.0173689\n",
      "[0]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 1.0173689\n",
      "[4]\n",
      "Best fitness is = 1.0173689\n",
      "Best parameters are:\n",
      "learning_rate 0.35\n",
      "n_estimators 30.0\n",
      "max_depth 4\n",
      "min_child_weight 9.66\n",
      "gamma 9.13\n",
      "subsample 0.74\n",
      "colsample_bytree 0.9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 4.2925426\n",
      "[1]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 4.54452\n",
      "[1]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 4.54452\n",
      "[6]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 4.54452\n",
      "[3]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 4.54452\n",
      "[1]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 4.54452\n",
      "[0]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 4.54452\n",
      "[4]\n",
      "Best fitness is = 4.54452\n",
      "Best parameters are:\n",
      "learning_rate 0.07\n",
      "n_estimators 19.0\n",
      "max_depth 6\n",
      "min_child_weight 6.37\n",
      "gamma 5.45\n",
      "subsample 0.77\n",
      "colsample_bytree 0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 1.5861019\n",
      "[6]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 1.7481876\n",
      "[0]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 1.7481876\n",
      "[1]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 2.2686337\n",
      "[3]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 2.3573419\n",
      "[5]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 2.3573419\n",
      "[0]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 2.3573419\n",
      "[5]\n",
      "Best fitness is = 2.3573419\n",
      "Best parameters are:\n",
      "learning_rate 0.51\n",
      "n_estimators 319.0\n",
      "max_depth 7\n",
      "min_child_weight 10.0\n",
      "gamma 7.39\n",
      "subsample 0.62\n",
      "colsample_bytree 0.49\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 1.6218639\n",
      "[4]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 1.6264925\n",
      "[1]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 1.6264925\n",
      "[2]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 1.6264925\n",
      "[1]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 1.6264925\n",
      "[5]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 1.6264925\n",
      "[6]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 1.6264925\n",
      "[2]\n",
      "Best fitness is = 1.6264925\n",
      "Best parameters are:\n",
      "learning_rate 0.59\n",
      "n_estimators 55.0\n",
      "max_depth 7\n",
      "min_child_weight 5.18\n",
      "gamma 2.58\n",
      "subsample 0.99\n",
      "colsample_bytree 0.88\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 4.3638628\n",
      "[3]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 4.3638628\n",
      "[5]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 4.7050358\n",
      "[3]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 4.7050358\n",
      "[1]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 4.7050358\n",
      "[0]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 4.7050358\n",
      "[1]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 4.7050358\n",
      "[6]\n",
      "Best fitness is = 4.7050358\n",
      "Best parameters are:\n",
      "learning_rate 0.09\n",
      "n_estimators 136.0\n",
      "max_depth 5\n",
      "min_child_weight 10.0\n",
      "gamma 8.6\n",
      "subsample 0.21000000000000002\n",
      "colsample_bytree 0.52\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "This is number 0 generation\n",
      "Best F1 score in the this iteration = 3.0734435\n",
      "[3]\n",
      "This is number 1 generation\n",
      "Best F1 score in the this iteration = 3.8483003\n",
      "[6]\n",
      "This is number 2 generation\n",
      "Best F1 score in the this iteration = 3.8483003\n",
      "[3]\n",
      "This is number 3 generation\n",
      "Best F1 score in the this iteration = 5.5658031\n",
      "[2]\n",
      "This is number 4 generation\n",
      "Best F1 score in the this iteration = 5.5658031\n",
      "[5]\n",
      "This is number 5 generation\n",
      "Best F1 score in the this iteration = 5.5658031\n",
      "[5]\n",
      "This is number 6 generation\n",
      "Best F1 score in the this iteration = 6.6636448\n",
      "[4]\n",
      "Best fitness is = 6.6636448\n",
      "Best parameters are:\n",
      "learning_rate 0.25\n",
      "n_estimators 179.0\n",
      "max_depth 10\n",
      "min_child_weight 10.0\n",
      "gamma 6.04\n",
      "subsample 0.12000000000000005\n",
      "colsample_bytree 0.12\n"
     ]
    }
   ],
   "source": [
    "numberOfParents = 10 # number of parents to start\n",
    "numberOfParentsMating = 4 # number of parents that will mate\n",
    "numberOfParameters = 7 # number of parameters that will be optimized\n",
    "numberOfGenerations = 7 # number of genration that will be created#define the population sizepopulationSize = (numberOfParents, numberOfParameters)#initialize the population with randomly generated parameters\n",
    "\n",
    "for trial_ind in np.arange(0,len(acceptable_trials)):\n",
    "    \n",
    "    TrainX = pandas.read_csv('./MLFormattedData/Train/' + acceptable_trials[trial_ind] + '_TrainX.csv' )\n",
    "    TrainY = pandas.read_csv('./MLFormattedData/Train/' + acceptable_trials[trial_ind] + '_TrainY.csv' )\n",
    "\n",
    "    TestX = pandas.read_csv('./MLFormattedData/Test/' + acceptable_trials[trial_ind] + '_TestX.csv' )\n",
    "    TestY = pandas.read_csv('./MLFormattedData/Test/' + acceptable_trials[trial_ind] + '_TestY.csv' )\n",
    "    \n",
    "    # For whatever reason, if we redefine this between trials, I think it will work fine. But if not, we get a list not callable error...?\n",
    "    def fitness(y_true, y_pred):\n",
    "        fitness = round((1/mean_squared_error(y_true, y_pred)), 7)\n",
    "        return fitness  # train the data and find fitness score; I modified fitness to 1/MSE -- therefore we still want the highest \"fitness\"\n",
    "\n",
    "\n",
    "    # define the population size\n",
    "    populationSize = (numberOfParents, numberOfParameters)\n",
    "\n",
    "    # initialize the population with randomly generated parameters\n",
    "    population = initilialize_poplulation(numberOfParents)#define an array to store the fitness  hitory\n",
    "    fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])#define an array to store the value of each parameter for each parent and generation\n",
    "    populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])#insert the value of initial parameters in history\n",
    "    populationHistory[0:numberOfParents, :] = population \n",
    "\n",
    "    for generation in range(numberOfGenerations):\n",
    "        print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "        #train the dataset and obtain fitness\n",
    "        fitnessValue = train_population(population=population, TrainX = TrainX, TrainY = TrainY, TestX = TestX, TestY = TestY)\n",
    "\n",
    "        fitnessHistory[generation, :] = fitnessValue\n",
    "\n",
    "        #best score in the current iteration\n",
    "        print('Highest (1/RMSE) score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))#survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
    "        parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
    "\n",
    "        #mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
    "        children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "\n",
    "        #add mutation to create genetic diversity\n",
    "        children_mutated = mutation(children, numberOfParameters)\n",
    "\n",
    "        '''\n",
    "        We will create new population, which will contain parents that where selected previously based on the\n",
    "        fitness score and rest of them  will be children\n",
    "        '''\n",
    "        population[0:parents.shape[0], :] = parents #fittest parents\n",
    "        population[parents.shape[0]:, :] = children_mutated #children\n",
    "\n",
    "        populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population #srore parent information\n",
    "        \n",
    "        \n",
    "    #Best solution from the final iteration\n",
    "    fitness = train_population(population=population, TrainX = TrainX, TrainY = TrainY, TestX = TestX, TestY =TestY)\n",
    "    fitnessHistory[generation+1, :] = fitness # index of the best solution\n",
    "    bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]#Best fitness\n",
    "    print(\"Highest 1/RMSE is =\", fitness[bestFitnessIndex])#Best parameters\n",
    "    print(\"Best parameters are:\")\n",
    "    print('learning_rate', population[bestFitnessIndex][0])\n",
    "    print('n_estimators', population[bestFitnessIndex][1])\n",
    "    print('max_depth', int(population[bestFitnessIndex][2])) \n",
    "    print('min_child_weight', population[bestFitnessIndex][3])\n",
    "    print('gamma', population[bestFitnessIndex][4])\n",
    "    print('subsample', population[bestFitnessIndex][5])\n",
    "    print('colsample_bytree', population[bestFitnessIndex][6])\n",
    "    \n",
    "    for param_ind in np.arange(0,7):\n",
    "        if param_ind == 2: \n",
    "            HypParams.at[trial_ind, column_names[param_ind]] = int(population[bestFitnessIndex][param_ind])\n",
    "        else:\n",
    "            HypParams.at[trial_ind, column_names[param_ind]] = population[bestFitnessIndex][param_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "HypParams.to_csv('GeneticAlgorithmParameters_Extrapolation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>gamma</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m07_t01_15</td>\n",
       "      <td>0.31</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m07_t03_15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m07_t06_15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.69</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m10_t02_16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.72</td>\n",
       "      <td>4.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m11_t02_16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.41</td>\n",
       "      <td>7.24</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>m11_t04_16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>9.13</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m12_t02_16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.37</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>m14_t05_16</td>\n",
       "      <td>0.51</td>\n",
       "      <td>319.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.39</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m14_t03_16</td>\n",
       "      <td>0.59</td>\n",
       "      <td>55.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.18</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>m15_t01_16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.60</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>m15_t03_16</td>\n",
       "      <td>0.25</td>\n",
       "      <td>179.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Trial  learning_rate  n_estimators  max_depth  min_child_weight  \\\n",
       "0   m07_t01_15           0.31         198.0        5.0             10.00   \n",
       "1   m07_t03_15           0.11          10.0        4.0             10.00   \n",
       "2   m07_t06_15           0.05          26.0        3.0              6.69   \n",
       "3   m10_t02_16           0.21          54.0        4.0              5.72   \n",
       "4   m11_t02_16           0.06         217.0        1.0              9.41   \n",
       "5   m11_t04_16           0.35          30.0        4.0              9.66   \n",
       "6   m12_t02_16           0.07          19.0        6.0              6.37   \n",
       "7   m14_t05_16           0.51         319.0        7.0             10.00   \n",
       "8   m14_t03_16           0.59          55.0        7.0              5.18   \n",
       "9   m15_t01_16           0.09         136.0        5.0             10.00   \n",
       "10  m15_t03_16           0.25         179.0       10.0             10.00   \n",
       "\n",
       "    gamma  subsample  colsample_bytree  \n",
       "0    7.95       0.85              0.91  \n",
       "1    0.61       0.63              0.92  \n",
       "2    7.42       0.76              0.13  \n",
       "3    4.68       0.67              0.79  \n",
       "4    7.24       1.00              0.36  \n",
       "5    9.13       0.74              0.90  \n",
       "6    5.45       0.77              0.40  \n",
       "7    7.39       0.62              0.49  \n",
       "8    2.58       0.99              0.88  \n",
       "9    8.60       0.21              0.52  \n",
       "10   6.04       0.12              0.12  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HypParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
