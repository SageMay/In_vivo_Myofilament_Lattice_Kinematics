{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score # This seems to only work for classification\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import math\n",
    "\n",
    "\n",
    "# Plot parameters \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font = {'family' : 'Tahoma',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 24}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the trials that we have good data for \n",
    "acceptable_trials = ['m07_t01_15', 'm07_t03_15', 'm07_t06_15','m10_t02_16','m11_t02_16','m11_t04_16',\n",
    "                   'm12_t02_16','m14_t05_16', 'm14_t03_16', 'm15_t01_16', 'm15_t03_16']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try some other hyperparameters\n",
    "# Hyperparameters to build the xGBoost regressor model\n",
    "obj ='reg:linear'\n",
    "csbt = .15 \n",
    "lr = .01\n",
    "md = 3\n",
    "n_est = 750\n",
    "ss = .7\n",
    "alp = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got it\n",
      "[16:48:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:01] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[16:48:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# I used this framework to determine if I could create predictions that were as good with fewer data columns.\n",
    "# My conclusion is that using all of these gives better predictions.\n",
    "\n",
    "exp_var = pandas.DataFrame((np.zeros((len(acceptable_trials),5)))*np.nan, index = [np.arange(0,len(acceptable_trials))], columns = ['Explained_variance', 'rms', 'mse', 'mse_with_mean', 'Number_estimators'])\n",
    "\n",
    "column_sets = [['M6_c', 'A59_c', 'M3_c', 'I20_I10', 'peaks']]\n",
    "\n",
    "diffcolumns_exp_var = pandas.DataFrame((np.zeros((len(column_sets),3)))*np.nan, index = [np.arange(0,len(column_sets))], columns = ['Explained_variance', 'mse', 'mse_with_mean'])\n",
    "print('got it')\n",
    "c_count = 0 \n",
    "for c in column_sets:\n",
    "    count = 0\n",
    "    exp_var = pandas.DataFrame((np.zeros((len(acceptable_trials),3)))*np.nan, index = [np.arange(0,len(acceptable_trials))], columns = ['Explained_variance', 'mse', 'mse_with_mean'])\n",
    "    for trial in acceptable_trials:\n",
    "        model_mse = 1\n",
    "        meanData_mse = 0\n",
    "        n_est = 500\n",
    "        #while model_mse > meanData_mse:\n",
    "        # Build the regressor object with optional kwargs -- this will be the same throughout. \n",
    "        xg_reg = xgb.XGBRegressor(objective =obj, colsample_bytree = csbt, learning_rate = lr,\n",
    "                        max_depth = md, n_estimators = n_est, subsample = ss, alpha = alp)\n",
    "\n",
    "        # Import data, interpolate nans and subset to desired columns\n",
    "        d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "        d = d.interpolate()\n",
    "        d = d[['D10', 'M6_c', 'I20_I10', 'M3_i', 'M3_c', 'A59_c', 'peaks','seconds']]\n",
    "\n",
    "            # Drop any rows that have a nan\n",
    "            #d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "\n",
    "            # Add time history in the form of time shifted columns\n",
    "            # I chose to  do this in a recursive style, referencing the j-1 shift and shifting it further by 1. It could be done without the recursion.\n",
    "        columns = c#column_sets[c] # ['M6_c', 'I20_I10'] # 'M3_i','A59_c','M6_c',\n",
    "        shift_column_names = []\n",
    "        for col in columns:\n",
    "            d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "            shift_column_names.append(col + '_' + str(0))\n",
    "\n",
    "        for j in np.arange(1,6):\n",
    "            for col in columns:\n",
    "                d[col + '_' + str(j)] = d[col + '_' + str(j-1)].shift(-1)\n",
    "                shift_column_names.append(col + '_' + str(j))\n",
    "\n",
    "        # This dropna will actually cut the total length of the dataframe since by time shifting there will be some columns that are nearly a whole ISI shorter than the original data length      \n",
    "        d = d.dropna()\n",
    "\n",
    "        # Subset the data into predictors (X) and predicted (y)\n",
    "        modeled_data = 'D10'\n",
    "        X = (d[columns + shift_column_names+ ['seconds']])\n",
    "        y = (d[[modeled_data]])\n",
    "\n",
    "        # Split the data into test and train\n",
    "        test_percent = .25 # Percentage of data to be reserved for test set. This needs to be kept as a chunk with time dependence\n",
    "        length_test_set = math.ceil(test_percent*len(d)) # Round up\n",
    "\n",
    "        test_X = X[:length_test_set]\n",
    "        test_y = y[:length_test_set]\n",
    "\n",
    "        train_X = X[length_test_set:]\n",
    "        train_y = y[length_test_set:]\n",
    "\n",
    "        test_y_mean = np.full((len(test_y)), np.mean(test_y))\n",
    "\n",
    "        # Fit the model\n",
    "        xg_reg.fit(train_X, train_y)\n",
    "\n",
    "        # Model prediction of the training set\n",
    "        y_pred = xg_reg.predict(train_X)\n",
    "\n",
    "        plt.rcParams['figure.figsize'] = [14, 6]   \n",
    "\n",
    "        # Model prediction of the test set\n",
    "        y_pred = xg_reg.predict(test_X)\n",
    "        exp_var.Explained_variance.iloc[count] = round(explained_variance_score(y_pred,test_y), 2)\n",
    "        #exp_var.rms.iloc[count] = round(((sum((np.concatenate(np.array(test_y)) - y_pred)**(2)))/len(y_pred))**(1/2), 3)\n",
    "        exp_var.mse.iloc[count] = round(mean_squared_error(y_pred, test_y), 2)\n",
    "        exp_var.mse_with_mean.iloc[count] = round(mean_squared_error(test_y_mean, test_y),2)\n",
    "        #exp_var.Number_estimators.iloc[count] = n_est\n",
    "        model_mse = round(mean_squared_error(y_pred, test_y), 2)\n",
    "        meanData_mse = round(mean_squared_error(test_y_mean, test_y),2)\n",
    "              \n",
    "        count += 1\n",
    "            \n",
    "    diffcolumns_exp_var.mse.iloc[c_count] = exp_var.mse.mean()\n",
    "    diffcolumns_exp_var.Explained_variance.iloc[c_count] = exp_var.Explained_variance.mean()\n",
    "    diffcolumns_exp_var.mse_with_mean.iloc[c_count] = exp_var.mse_with_mean.mean()\n",
    "        \n",
    "    c_count += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explained_variance</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_with_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.309091</td>\n",
       "      <td>0.263636</td>\n",
       "      <td>0.283636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Explained_variance       mse  mse_with_mean\n",
       "0           -4.309091  0.263636       0.283636"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffcolumns_exp_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to do the same thing in essence -- what dat are needed to create a good prediction\n",
    "# except I will do this on the train on n-1 trials and test on the nth. \n",
    "\n",
    "# Split data into test and train sets\n",
    "acceptable_trials = ['m07_t01_15', 'm07_t03_15', 'm07_t06_15','m10_t02_16','m11_t02_16','m11_t04_16',\n",
    "                   'm12_t02_16','m14_t05_16', 'm14_t03_16', 'm15_t01_16', 'm15_t03_16']\n",
    "\n",
    "# Split the data into test and train sets\n",
    "test_trials = []\n",
    "train_trials = []\n",
    "for i in np.arange(0,len(acceptable_trials)):\n",
    "    test_trials.append(acceptable_trials[i])\n",
    "    train_trials.append((acceptable_trials[:(i)] + acceptable_trials[(i+1):]))\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:37:11] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:13] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:17] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:22] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:24] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:26] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:29] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:31] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:33] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:40] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:45] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:50] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:37:55] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:38:00] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:38:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:38:10] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:38:15] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:38:20] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[17:38:25] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "# train on N-1 of N trials and test on the removed trial\n",
    "column_sets = [['I20_I10'], ['M6_c', 'I20_I10', 'peaks']]\n",
    "test_column = 'D10'\n",
    "#predictor_columns = ['Wingbeat_freq', 'M6_c', 'I20_I10', 'M3_i', 'A59_c']\n",
    "#exp_var_all_trials = pandas.DataFrame((np.zeros((len(acceptable_trials),3)))*np.nan, index = [np.arange(0,len(acceptable_trials))], columns = ['Explained_variance', 'mse', 'mse_with_mean'])\n",
    "exp_var_all_trials_all_col_sets = pandas.DataFrame((np.zeros((len(column_sets),3)))*np.nan, index = [np.arange(0,len(column_sets))], columns = ['Explained_variance', 'mse', 'mse_with_mean'])\n",
    "\n",
    "n_est = 1850\n",
    "col_count = 0\n",
    "for c in column_sets:\n",
    "    exp_var_all_trials = pandas.DataFrame((np.zeros((len(acceptable_trials),3)))*np.nan, index = [np.arange(0,len(acceptable_trials))], columns = ['Explained_variance', 'mse', 'mse_with_mean'])\n",
    "    for i in np.arange(0,len(train_trials)): \n",
    "        xg_reg = xgb.XGBRegressor(objective =obj, colsample_bytree = csbt, learning_rate = lr,\n",
    "                    max_depth = md, n_estimators = n_est, subsample = ss, alpha = alp)\n",
    "\n",
    "\n",
    "        # Prep the data\n",
    "\n",
    "        # Base case\n",
    "        trial = train_trials[i][0]\n",
    "        d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "        #d = d[['D10', 'D11', 'D20', 'I20I11_I10', 'I20_I10',\n",
    "        #       'fitting_error', 'seconds', 'tif_num', 'M3_c', 'M6_c', 'A51_c', 'A59_c',\n",
    "        #       'M3_i', 'M6_i', 'A59_i', 'A51_i', 'Sum', 'peaks', 'ISI']]\n",
    "\n",
    "        d = d.interpolate() # Fill any nan values\n",
    "        d = d[['D10', 'I20_I10', 'M3_i', 'A59_c', 'M6_c', 'peaks']]\n",
    "        #d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "\n",
    "        columns = c#['M3_i', 'M6_c', 'I20_I10', 'A59_c']\n",
    "        shift_column_names = []\n",
    "        for col in columns:\n",
    "            d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "            shift_column_names.append(col + '_' + str(0))\n",
    "\n",
    "        for j in np.arange(1,11):\n",
    "            for col in columns:\n",
    "                d[col + '_' + str(j)] = d[col + '_' + str(j - 1)].shift(-1)\n",
    "                shift_column_names.append(col + '_' + str(j))\n",
    "\n",
    "        d = d.dropna()\n",
    "\n",
    "        # Subset the data into predictors (X) and predicted (y)\n",
    "        X = (d[columns + shift_column_names])\n",
    "        y = (d[['D10']])\n",
    "\n",
    "\n",
    "        # Subsequent cases\n",
    "\n",
    "        # Create training set\n",
    "        for trial in train_trials[i][1:]: \n",
    "            d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "            d = d[['D10', 'D11', 'D20', 'I20I11_I10', 'I20_I10',\n",
    "                   'fitting_error', 'seconds', 'tif_num', 'M3_c', 'M6_c', 'A51_c', 'A59_c',\n",
    "                   'M3_i', 'M6_i', 'A59_i', 'A51_i', 'Sum', 'peaks', 'ISI']]\n",
    "\n",
    "            d = d.interpolate()\n",
    "            d = d[['D10', 'I20_I10', 'M3_i', 'A59_c', 'M6_c', 'peaks']]\n",
    "            #d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "\n",
    "            for col in columns:\n",
    "                d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "                #shift_column_names.append(col + '_' + str(0))\n",
    "\n",
    "            for j in np.arange(1,11):\n",
    "                for col in columns:\n",
    "                    d[col + '_' + str(j)] = d[col + '_' + str(j - 1)].shift(-1)\n",
    "\n",
    "            d = d.dropna()\n",
    "\n",
    "            # Subset the data into predictors (X) and predicted (y)\n",
    "            X = X.append(d[columns + shift_column_names])\n",
    "            y = y.append(d[['D10']])\n",
    "\n",
    "        X = X.reset_index(drop = 'True')\n",
    "        y = y.reset_index(drop = 'True')\n",
    "\n",
    "        xg_reg.fit(X, y)\n",
    "        y_pred = xg_reg.predict(X)\n",
    "\n",
    "        # Use the last df as a test set\n",
    "        trial = test_trials[i]\n",
    "        d = pandas.read_csv('../DataProcessing/ProcessedData/' + trial + '_det.csv')\n",
    "        #d = d[['D10', 'D11', 'D20', 'I20I11_I10', 'I20_I10',\n",
    "        #       'fitting_error', 'seconds', 'tif_num', 'M3_c', 'M6_c', 'A51_c', 'A59_c',\n",
    "        #       'M3_i', 'M6_i', 'A59_i', 'A51_i', 'Sum', 'peaks', 'ISI']]\n",
    "        d = d.interpolate()\n",
    "        d = d[['D10', 'I20_I10', 'M3_i', 'A59_c', 'M6_c', 'peaks']]\n",
    "        d = d.dropna(how = 'any').reset_index(drop = 'True')\n",
    "\n",
    "        # Create time shifted predictor data\n",
    "        for col in columns:\n",
    "            d[col + '_' + str(0)] = d[col].shift(-1)\n",
    "        for j in np.arange(1,11):\n",
    "            for col in columns:\n",
    "                d[col + '_' + str(j)] = d[col + '_' + str(j - 1)].shift(-1)\n",
    "        d = d.dropna()\n",
    "\n",
    "        X = (d[columns + shift_column_names])\n",
    "        y = (d[['D10']])\n",
    "        y_mean = np.full((len(y)), np.mean(y))\n",
    "\n",
    "\n",
    "        # Use the trained regressor to predict y from the test set X\n",
    "        y_pred = xg_reg.predict(X)\n",
    "\n",
    "        # Calculate and store error metrics (explained variance, a manual root mean square and mean square error from scikitlearn)\n",
    "        exp_var_all_trials.Explained_variance.iloc[i] = explained_variance_score(y_pred,y)\n",
    "        exp_var_all_trials.mse.iloc[i] = mean_squared_error(y_pred,y)\n",
    "        exp_var_all_trials.mse_with_mean.iloc[i] = mean_squared_error(y_mean, y)\n",
    "        \n",
    "        #plt.rcParams['figure.figsize'] = [14, 6]\n",
    "        #fig = plt.figure()\n",
    "        #ax = fig.add_subplot(111)\n",
    "        #ax.plot((np.arange(0,len(y))*(1/200))[0:50], y[0:50], c = 'black', label = 'Target')\n",
    "        #ax.plot((np.arange(0,len(y))*(1/200))[0:50], y_mean[0:50], c = 'black', alpha = .5, label = 'Target mean')\n",
    "        #ax.scatter((np.arange(0,len(y))*(1/200))[0:50], y_pred[0:50], c = 'black', marker = 'x', label = 'Prediction')\n",
    "        #ax.legend(loc = 'upper right')\n",
    "        #ax.set_title('Prediction of lattice spacing change in time')\n",
    "        #ax.set_ylabel('Lattice spacing (percent change)')#modeled_data)\n",
    "        #ax.set_xlabel('Time (s)')\n",
    "        #ax.text(0.02,.9, 'Explained variance score: ' + str(round(explained_variance_score(y_pred,y), 2)), family=\"sans-serif\", fontsize = 20, transform=ax.transAxes) # 0.03, 2.,\n",
    "        #ax.yaxis.set_major_locator(plt.MaxNLocator(4)) \n",
    "        #ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        #plt.savefig('/Users/sagemalingen/Desktop/RandomForest_Figs/Predicted_AllTrialTrain_' + test_trials[i] + '.png', dpi = 400)\n",
    "        #plt.show()\n",
    "    exp_var_all_trials_all_col_sets.mse.iloc[col_count] = exp_var_all_trials.mse.mean()\n",
    "    exp_var_all_trials_all_col_sets.mse_with_mean.iloc[col_count] = exp_var_all_trials.mse_with_mean.mean()\n",
    "    exp_var_all_trials_all_col_sets.Explained_variance.iloc[col_count] = exp_var_all_trials.Explained_variance.mean()\n",
    "    col_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explained_variance</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_with_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-187.308610</td>\n",
       "      <td>1.181208</td>\n",
       "      <td>0.270265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7.530685</td>\n",
       "      <td>0.812408</td>\n",
       "      <td>0.270265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.481175</td>\n",
       "      <td>0.939539</td>\n",
       "      <td>0.270265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.287844</td>\n",
       "      <td>0.925092</td>\n",
       "      <td>0.270265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Explained_variance       mse  mse_with_mean\n",
       "0         -187.308610  1.181208       0.270265\n",
       "1           -7.530685  0.812408       0.270265\n",
       "2           -8.481175  0.939539       0.270265\n",
       "3           -7.287844  0.925092       0.270265"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_var_all_trials_all_col_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technically is best with only I20_I10, but very similar to having that plus other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explained_variance</th>\n",
       "      <th>mse</th>\n",
       "      <th>mse_with_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.942015</td>\n",
       "      <td>0.698027</td>\n",
       "      <td>0.270265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.421442</td>\n",
       "      <td>0.915192</td>\n",
       "      <td>0.270265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Explained_variance       mse  mse_with_mean\n",
       "0           -1.942015  0.698027       0.270265\n",
       "1           -2.421442  0.915192       0.270265"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_var_all_trials_all_col_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
